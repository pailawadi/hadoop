Sqoop is a tool which is part of CDH . 
Documentation for Sqoop : https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_connecting_to_a_database_server

Benifits of Using Sqoop: Sqoop is used to import export data from relational databases into HDFS , while preserving struncture.

to see all command available in sqoop . 

==================================================
> sqoop help
Available commands in sqoop:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  import-mainframe   Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information

===============================================
To get help on specific sqoop command use : sqoop help import 

=======================================================
Command Usage (All commands are run in context of CLOUDERA Quick Start VM )
To list all databases available in a mysql database 
> sqoop list-databases --connect "jdbc:mysql://quickstart.cloudera:3306" --username retail_dba --password cloudera

==output of the above command
> 16/11/16 07:04:34 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
information_schema
retail_db

===============================================
To List all tables in a database
> sqoop list-tables --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba --password cloudera

==output of the above command:Displays all tables in retail_db

line is insecure. Consider using -P instead.
16/11/16 07:11:22 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
categories
customers
departments
order_items
orders
products
===============================================
To run a query using eval: Before running quesrites this is just a sanity check if user have required permissions to execute queries on 
a database or not.

sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba --password cloudera --query "select * from departments"
==output
16/11/16 11:53:09 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
--------------------------------------
| department_id | department_name      | 
--------------------------------------
| 2           | Fitness              | 
| 3           | Footwear             | 
| 4           | Apparel              | 
| 5           | Golf                 | 
| 6           | Outdoors             | 
| 7           | Fan Shop             | 
--------------------------------------

============= ===========================================

To Hide a passowrd : User -P as argument which will prompt for password . (Username and password is of database to connect in this case mysql)

sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba -P  --query "select * from departments"

============================
4) Sqoop Import: To import tables , command used is sqoop import. Sqoop Import takes two types of arguments generic and specific arguments
Generic arguments containg : --connect , --username --password, --

Tool Specific arguments are followed by -- ex: --connect , hadoop arguments are followed by - for ex -fs ,-D ,-jt

> sqoop import-all-tables
  -m (number of mappers /parallel thread )
  --connect "jdbc://"
  --username ={}
  --password={}
  --as{-avrodatafile} ( file format in which data need to be stored in HDFS, this represents avro file format  which is similar to JSON. Default file format is text file ,also there is binary file format
  --warehouse-dir ={location of directory in hdfs where data to be stored}
  
 
example: sqoop import-all-tables -m 12  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba --password cloudera --warehouse-dir "/user/cloudera/sqoop_import"

to check if data is copied : check hadoop file system : hadoop fs -ls /user/cloudera/sqoop_import/ will show all the tables
to check specific table data : /user/cloudera/sqoop_import/products will show details of product table

to look ast data hadoop fs -tail  file path in hdfs:

1343,59,Nike Men's Home Game Jersey St. Louis Rams Mi,,100.0,http://images.acmesports.sports/Nike+Men%27s+Home+Game+Jersey+St.+Louis+Rams+Michael+Sam+%2396
1344,59,Nike Men's Home Game Jersey St. Louis Rams Aa,,100.0,http://images.acmesports.sports/Nike+Men%27s+Home+Game+Jersey+St.+Louis+Rams+Aaron+Donald+%2399

hadoop fs -cat to see content of a file

hadoop fs -wc -l * 
and verify these counts by running count query on sql runnng sqoop eval commands

============================================
How to move data to Hive table from MySql using sqoop
When running hive in a cluster , there will be a directory in hdfs  /user/hive/warehouse where all hive data is stored in hdfs
Hive uses mapreduce as processing engine and hdfs as storage
we created directory named: /user/hive/warehouse/sqoop_import database .

sqoop import-all-tables \
> --num-mappers 1 \
> --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
> --username retail_dba --password cloudera \
> --hive-import --hive-overwrite \
> --create-hive-table --compress \
> --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
> --outdir java_files
